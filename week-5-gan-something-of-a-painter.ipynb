{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom PIL import Image\nimport pandas as pd \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.212036Z","iopub.execute_input":"2025-07-31T03:57:33.212611Z","iopub.status.idle":"2025-07-31T03:57:33.216740Z","shell.execute_reply.started":"2025-07-31T03:57:33.212586Z","shell.execute_reply":"2025-07-31T03:57:33.215922Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**Brief description of the problem and data (5 pts)**\n\nBriefly describe the challenge problem and NLP. Describe the size, dimension, structure, etc., of the data.\n\nThis competition is based around Generative Adversarial Networks. In essence, in this instance, you create two networks. one that can tell the difference  between a real image, and a fake. And another that creates fake images. In this case there is a twist, and that is it will take an image, and try and create the same basic image, but as a monet painting. The basic data structure contains four folders, two of them, sets of image in JPEG format (monet(300), and photos(7038)), the second are the same images in TfRec format (guessing its a tensorflow ready file format). There are no sample output files, the idea is that when the model is finished it will output between 7-10K monet style (at least thransform the 7038 files in photos to their monet equivalents, add more if desired).","metadata":{}},{"cell_type":"code","source":"# Paths to simplify navigation. \nDATA = '/kaggle/input/gan-getting-started/'\nMONET_JPG = os.path.join(DATA, 'monet_jpg')\nPHOTO_JPG = os.path.join(DATA, 'photo_jpg')\nMONET_TFREC = os.path.join(DATA, 'monet_tfrec')\nPHOTO_TFREC = os.path.join(DATA, 'photo_tfrec')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.217748Z","iopub.execute_input":"2025-07-31T03:57:33.217978Z","iopub.status.idle":"2025-07-31T03:57:33.231458Z","shell.execute_reply.started":"2025-07-31T03:57:33.217959Z","shell.execute_reply":"2025-07-31T03:57:33.230846Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"First we will load the photos normally so we can analyse them. ","metadata":{}},{"cell_type":"code","source":"'''monet_files = sorted(glob.glob(os.path.join(MONET_JPG, '*.jpg')))\nphoto_files = sorted(glob.glob(os.path.join(PHOTO_JPG, '*.jpg')))\n\nprint(f\"Number of Monet images: {len(monet_files)}\")\nprint(f\"Number of Photo images: {len(photo_files)}\")\n\n# Load and display a sample Monet image\nsample_monet = load_img(monet_files[0])\nplt.imshow(sample_monet)\nplt.title(\"Sample Monet Painting\")\nplt.axis('off')\nplt.show()\n\n# Similarly for a photo\nsample_photo = load_img(photo_files[0])\nplt.imshow(sample_photo)\nplt.title(\"Sample Photo\")\nplt.axis('off')\nplt.show()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.232598Z","iopub.execute_input":"2025-07-31T03:57:33.232809Z","iopub.status.idle":"2025-07-31T03:57:33.246577Z","shell.execute_reply.started":"2025-07-31T03:57:33.232791Z","shell.execute_reply":"2025-07-31T03:57:33.245953Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'monet_files = sorted(glob.glob(os.path.join(MONET_JPG, \\'*.jpg\\')))\\nphoto_files = sorted(glob.glob(os.path.join(PHOTO_JPG, \\'*.jpg\\')))\\n\\nprint(f\"Number of Monet images: {len(monet_files)}\")\\nprint(f\"Number of Photo images: {len(photo_files)}\")\\n\\n# Load and display a sample Monet image\\nsample_monet = load_img(monet_files[0])\\nplt.imshow(sample_monet)\\nplt.title(\"Sample Monet Painting\")\\nplt.axis(\\'off\\')\\nplt.show()\\n\\n# Similarly for a photo\\nsample_photo = load_img(photo_files[0])\\nplt.imshow(sample_photo)\\nplt.title(\"Sample Photo\")\\nplt.axis(\\'off\\')\\nplt.show()'"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"We are going to attempt to use the TFRecs in the model pipeline in hopes of faster training as I am very close to out of GPU time on my account. So here goes nothing!","metadata":{}},{"cell_type":"code","source":"'''# Parsing function (from earlier)\ndef parse_tfrecord(example):\n    features = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string)\n    }\n    parsed = tf.io.parse_single_example(example, features)\n    image = tf.io.decode_jpeg(parsed['image'], channels=3)\n    image = tf.image.resize(image, [256, 256])\n    image = (tf.cast(image, tf.float32) / 127.5) - 1.0  # [-1,1] norm for GAN stability\n    return image\n\n# Augmentation function (simple for now)\ndef augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    return image\n\n# Build datasets\nBATCH_SIZE = 1  # CycleGAN often uses 1 for instance norm; increase if hardware allows\nBUFFER_SIZE = 300  # For shuffling Monets (small set)\n\nmonet_tfrec_files = sorted(glob.glob(os.path.join(MONET_TFREC, '*.tfrec')))\nphoto_tfrec_files = sorted(glob.glob(os.path.join(PHOTO_TFREC, '*.tfrec')))\n\nmonet_ds = tf.data.TFRecordDataset(monet_tfrec_files)\nmonet_ds = monet_ds.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\nmonet_ds = monet_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\nmonet_ds = monet_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nphoto_ds = tf.data.TFRecordDataset(photo_tfrec_files)\nphoto_ds = photo_ds.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\nphoto_ds = photo_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\nphoto_ds = photo_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # Larger shuffle for photos\n\n# Zip in single dataset\ntrain_ds = tf.data.Dataset.zip((monet_ds, photo_ds))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.247289Z","iopub.execute_input":"2025-07-31T03:57:33.247526Z","iopub.status.idle":"2025-07-31T03:57:33.262944Z","shell.execute_reply.started":"2025-07-31T03:57:33.247509Z","shell.execute_reply":"2025-07-31T03:57:33.262413Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\"# Parsing function (from earlier)\\ndef parse_tfrecord(example):\\n    features = {\\n        'image': tf.io.FixedLenFeature([], tf.string),\\n        'image_name': tf.io.FixedLenFeature([], tf.string)\\n    }\\n    parsed = tf.io.parse_single_example(example, features)\\n    image = tf.io.decode_jpeg(parsed['image'], channels=3)\\n    image = tf.image.resize(image, [256, 256])\\n    image = (tf.cast(image, tf.float32) / 127.5) - 1.0  # [-1,1] norm for GAN stability\\n    return image\\n\\n# Augmentation function (simple for now)\\ndef augment(image):\\n    image = tf.image.random_flip_left_right(image)\\n    image = tf.image.random_flip_up_down(image)\\n    return image\\n\\n# Build datasets\\nBATCH_SIZE = 1  # CycleGAN often uses 1 for instance norm; increase if hardware allows\\nBUFFER_SIZE = 300  # For shuffling Monets (small set)\\n\\nmonet_tfrec_files = sorted(glob.glob(os.path.join(MONET_TFREC, '*.tfrec')))\\nphoto_tfrec_files = sorted(glob.glob(os.path.join(PHOTO_TFREC, '*.tfrec')))\\n\\nmonet_ds = tf.data.TFRecordDataset(monet_tfrec_files)\\nmonet_ds = monet_ds.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\\nmonet_ds = monet_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\\nmonet_ds = monet_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\\n\\nphoto_ds = tf.data.TFRecordDataset(photo_tfrec_files)\\nphoto_ds = photo_ds.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\\nphoto_ds = photo_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\\nphoto_ds = photo_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  # Larger shuffle for photos\\n\\n# Zip in single dataset\\ntrain_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\""},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"'''for monet, photo in train_ds.take(1):\n    # Visualize Monet sample (denormalize from [-1,1] to [0,1] for display)\n    plt.figure(figsize=(10, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow((monet[0] + 1) / 2)\n    plt.title(\"Sample Monet\")\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow((photo[0] + 1) / 2)\n    plt.title(\"Sample Photo\")\n    plt.axis('off')\n    \n    plt.show()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.264164Z","iopub.execute_input":"2025-07-31T03:57:33.264346Z","iopub.status.idle":"2025-07-31T03:57:33.280808Z","shell.execute_reply.started":"2025-07-31T03:57:33.264332Z","shell.execute_reply":"2025-07-31T03:57:33.280104Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'for monet, photo in train_ds.take(1):\\n    # Visualize Monet sample (denormalize from [-1,1] to [0,1] for display)\\n    plt.figure(figsize=(10, 5))\\n    \\n    plt.subplot(1, 2, 1)\\n    plt.imshow((monet[0] + 1) / 2)\\n    plt.title(\"Sample Monet\")\\n    plt.axis(\\'off\\')\\n    \\n    plt.subplot(1, 2, 2)\\n    plt.imshow((photo[0] + 1) / 2)\\n    plt.title(\"Sample Photo\")\\n    plt.axis(\\'off\\')\\n    \\n    plt.show()'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"'''# Verify shapes and norms\nprint(\"Monet shape:\", monet.shape)  # Should be (1, 256, 256, 3)\nprint(\"Photo shape:\", photo.shape)  # Should be (1, 256, 256, 3)\nprint(\"Monet min/max:\", tf.reduce_min(monet).numpy(), tf.reduce_max(monet).numpy())  # Expect ~ -1 to 1\nprint(\"Photo min/max:\", tf.reduce_min(photo).numpy(), tf.reduce_max(photo).numpy())  # Expect ~ -1 to 1'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.281578Z","iopub.execute_input":"2025-07-31T03:57:33.281780Z","iopub.status.idle":"2025-07-31T03:57:33.297448Z","shell.execute_reply.started":"2025-07-31T03:57:33.281758Z","shell.execute_reply":"2025-07-31T03:57:33.296835Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'# Verify shapes and norms\\nprint(\"Monet shape:\", monet.shape)  # Should be (1, 256, 256, 3)\\nprint(\"Photo shape:\", photo.shape)  # Should be (1, 256, 256, 3)\\nprint(\"Monet min/max:\", tf.reduce_min(monet).numpy(), tf.reduce_max(monet).numpy())  # Expect ~ -1 to 1\\nprint(\"Photo min/max:\", tf.reduce_min(photo).numpy(), tf.reduce_max(photo).numpy())  # Expect ~ -1 to 1'"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"TFRec data has been imported, and the train dataset file apears to be working correctly. We will move onto EDA and see if there is anything we can find to analyse the picture data given.","metadata":{}},{"cell_type":"markdown","source":"**Exploratory Data Analysis (EDA) â€” Inspect, Visualize and Clean the Data (15 pts)**\n\nShow a few visualizations like histograms. Describe any data cleaning procedures. Based on your EDA, what is your plan of analysis? ","metadata":{}},{"cell_type":"markdown","source":"Next we will inspect the data, check for invalid files, verify picture size is all uniform, visualize a few pictures from each, and maybe plot a few histograms.","metadata":{}},{"cell_type":"code","source":"'''def check_images(files):\n    invalid = []\n    for f in files:\n        try:\n            img = Image.open(f)\n            img.verify()\n            img.close()\n        except (IOError, SyntaxError):\n            invalid.append(f)\n    return invalid\n\ninvalid_monet = check_images(monet_files)\ninvalid_photo = check_images(photo_files)\nprint(f\"Invalid Monet: {invalid_monet}\")\nprint(f\"Invalid Photos: {invalid_photo}\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.298173Z","iopub.execute_input":"2025-07-31T03:57:33.298388Z","iopub.status.idle":"2025-07-31T03:57:33.311796Z","shell.execute_reply.started":"2025-07-31T03:57:33.298366Z","shell.execute_reply":"2025-07-31T03:57:33.311029Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'def check_images(files):\\n    invalid = []\\n    for f in files:\\n        try:\\n            img = Image.open(f)\\n            img.verify()\\n            img.close()\\n        except (IOError, SyntaxError):\\n            invalid.append(f)\\n    return invalid\\n\\ninvalid_monet = check_images(monet_files)\\ninvalid_photo = check_images(photo_files)\\nprint(f\"Invalid Monet: {invalid_monet}\")\\nprint(f\"Invalid Photos: {invalid_photo}\")'"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"Next we will verify picture dimensions.","metadata":{}},{"cell_type":"code","source":"'''def verify_dimensions(files):\n    for f in files:\n        img = load_img(f)\n        arr = img_to_array(img)\n        if arr.shape != (256, 256, 3):\n            print(f\"Invalid shape for {f}: {arr.shape}\")\n\nverify_dimensions(monet_files[:len(monet_files)])'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.312575Z","iopub.execute_input":"2025-07-31T03:57:33.312835Z","iopub.status.idle":"2025-07-31T03:57:33.326316Z","shell.execute_reply.started":"2025-07-31T03:57:33.312815Z","shell.execute_reply":"2025-07-31T03:57:33.325700Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'def verify_dimensions(files):\\n    for f in files:\\n        img = load_img(f)\\n        arr = img_to_array(img)\\n        if arr.shape != (256, 256, 3):\\n            print(f\"Invalid shape for {f}: {arr.shape}\")\\n\\nverify_dimensions(monet_files[:len(monet_files)])'"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"No problems there, all files (256, 256, 3). Next we will plot out a grid of pictures from each set to get a better idea whnat tghe datasets consist of.","metadata":{}},{"cell_type":"code","source":"'''def plot_grid(files, title, n=25):\n    plt.figure(figsize=(10, 10))\n    for i, f in enumerate(files[:n]):\n        plt.subplot(5, 5, i+1)\n        img = load_img(f)\n        plt.imshow(img)\n        plt.axis('off')\n    plt.suptitle(title)\n    plt.show()\n\nplot_grid(monet_files, \"Monet Paintings Grid\")\nplot_grid(photo_files, \"Photos Grid\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.326948Z","iopub.execute_input":"2025-07-31T03:57:33.327195Z","iopub.status.idle":"2025-07-31T03:57:33.339165Z","shell.execute_reply.started":"2025-07-31T03:57:33.327172Z","shell.execute_reply":"2025-07-31T03:57:33.338569Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'def plot_grid(files, title, n=25):\\n    plt.figure(figsize=(10, 10))\\n    for i, f in enumerate(files[:n]):\\n        plt.subplot(5, 5, i+1)\\n        img = load_img(f)\\n        plt.imshow(img)\\n        plt.axis(\\'off\\')\\n    plt.suptitle(title)\\n    plt.show()\\n\\nplot_grid(monet_files, \"Monet Paintings Grid\")\\nplot_grid(photo_files, \"Photos Grid\")'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"And finally we will plot a color histogram to see if there is anything that stands out about the monet paintings vs the real photos.","metadata":{}},{"cell_type":"code","source":"'''def plot_histogram(files, title, color='rgb', n=50, clip_max=22000):\n    images = np.array([img_to_array(load_img(f)) for f in files[:n]])\n    plt.figure()\n    for i, c in enumerate(color):\n        hist = np.histogram(images[..., i].flatten(), bins=256)[0]\n        if clip_max is not None:\n            hist = np.clip(hist, 0, clip_max) \n        plt.plot(hist, color=c)\n    plt.title(title)\n    if clip_max is not None:\n        plt.ylim(0, clip_max)\n    plt.show()\n\nplot_histogram(monet_files, \"Monet Histogram\")\nplot_histogram(photo_files, \"Photo Histogram\")'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.417023Z","iopub.execute_input":"2025-07-31T03:57:33.417274Z","iopub.status.idle":"2025-07-31T03:57:33.421951Z","shell.execute_reply.started":"2025-07-31T03:57:33.417257Z","shell.execute_reply":"2025-07-31T03:57:33.421379Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'def plot_histogram(files, title, color=\\'rgb\\', n=50, clip_max=22000):\\n    images = np.array([img_to_array(load_img(f)) for f in files[:n]])\\n    plt.figure()\\n    for i, c in enumerate(color):\\n        hist = np.histogram(images[..., i].flatten(), bins=256)[0]\\n        if clip_max is not None:\\n            hist = np.clip(hist, 0, clip_max) \\n        plt.plot(hist, color=c)\\n    plt.title(title)\\n    if clip_max is not None:\\n        plt.ylim(0, clip_max)\\n    plt.show()\\n\\nplot_histogram(monet_files, \"Monet Histogram\")\\nplot_histogram(photo_files, \"Photo Histogram\")'"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"One think I find interesting from these histograms is very few blacks or bright colors in the monets, and plenty of blacks in the photos. ","metadata":{}},{"cell_type":"markdown","source":"**DModel Architecture (25 pts)**\n\nDescribe your model architecture and reasoning for why you believe that specific architecture would be suitable for this problem. Compare multiple architectures and tune hyperparameters. ","metadata":{}},{"cell_type":"markdown","source":"My model will consist of a CycleGan: two generators(photo -> monet, Monet -> photo) and two discriminators using the Tensorflow CycleGan tutorial for reference","metadata":{}},{"cell_type":"code","source":"'''\nfrom tensorflow.keras import layers\n\ndef downsample(filters, size, apply_norm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(layers.LayerNormalization())  \n    result.add(layers.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\n    result.add(layers.LayerNormalization())\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    result.add(layers.ReLU())\n    return result\n\n# Generator: U-Net like\ndef Generator():\n    inputs = layers.Input(shape=[256, 256, 3])\n    \n    down_stack = [\n        downsample(64, 4, apply_norm=False),  # (bs, 128, 128, 64)\n        downsample(128, 4),  # (bs, 64, 64, 128)\n        downsample(256, 4),  # (bs, 32, 32, 256)\n        downsample(512, 4),  # (bs, 16, 16, 512)\n        downsample(512, 4),  # (bs, 8, 8, 512)\n        downsample(512, 4),  # (bs, 4, 4, 512)\n        downsample(512, 4),  # (bs, 2, 2, 512)\n        downsample(512, 4),  # (bs, 1, 1, 512)\n    ]\n    \n    up_stack = [\n        upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n        upsample(512, 4),  # (bs, 16, 16, 1024)\n        upsample(256, 4),  # (bs, 32, 32, 512)\n        upsample(128, 4),  # (bs, 64, 64, 256)\n        upsample(64, 4),   # (bs, 128, 128, 128)\n    ]\n    \n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=tf.random_normal_initializer(0., 0.02), activation='tanh')  # (bs, 256, 256, 3)\n    \n    x = inputs\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n    \n    skips = reversed(skips[:-1])\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n    \n    x = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\n# Discriminator: PatchGAN\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inp = layers.Input(shape=[256, 256, 3])\n    x = downsample(64, 4, False)(inp)  \n    x = downsample(128, 4)(x) \n    x = downsample(256, 4)(x)  \n    x = layers.ZeroPadding2D()(x)\n    x = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)  # (bs, 31, 31, 512)\n    x = layers.LayerNormalization()(x)\n    x = layers.LeakyReLU()(x)\n    x = layers.ZeroPadding2D()(x)\n    x = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)  # (bs, 30, 30, 1)\n    return tf.keras.Model(inputs=inp, outputs=x)\n\n# Instantiate\n# Photo to Monet\ngen_G = Generator()  \n# Monet to Photo\ngen_F = Generator()  \n# For photos\ndisc_X = Discriminator() \n# For Monets\ndisc_Y = Discriminator()\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.424211Z","iopub.execute_input":"2025-07-31T03:57:33.424706Z","iopub.status.idle":"2025-07-31T03:57:33.439288Z","shell.execute_reply.started":"2025-07-31T03:57:33.424677Z","shell.execute_reply":"2025-07-31T03:57:33.438688Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"\"\\nfrom tensorflow.keras import layers\\n\\ndef downsample(filters, size, apply_norm=True):\\n    initializer = tf.random_normal_initializer(0., 0.02)\\n    result = tf.keras.Sequential()\\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\\n    if apply_norm:\\n        result.add(layers.LayerNormalization())  \\n    result.add(layers.LeakyReLU())\\n    return result\\n\\ndef upsample(filters, size, apply_dropout=False):\\n    initializer = tf.random_normal_initializer(0., 0.02)\\n    result = tf.keras.Sequential()\\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', kernel_initializer=initializer, use_bias=False))\\n    result.add(layers.LayerNormalization())\\n    if apply_dropout:\\n        result.add(layers.Dropout(0.5))\\n    result.add(layers.ReLU())\\n    return result\\n\\n# Generator: U-Net like\\ndef Generator():\\n    inputs = layers.Input(shape=[256, 256, 3])\\n    \\n    down_stack = [\\n        downsample(64, 4, apply_norm=False),  # (bs, 128, 128, 64)\\n        downsample(128, 4),  # (bs, 64, 64, 128)\\n        downsample(256, 4),  # (bs, 32, 32, 256)\\n        downsample(512, 4),  # (bs, 16, 16, 512)\\n        downsample(512, 4),  # (bs, 8, 8, 512)\\n        downsample(512, 4),  # (bs, 4, 4, 512)\\n        downsample(512, 4),  # (bs, 2, 2, 512)\\n        downsample(512, 4),  # (bs, 1, 1, 512)\\n    ]\\n    \\n    up_stack = [\\n        upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\\n        upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\\n        upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\\n        upsample(512, 4),  # (bs, 16, 16, 1024)\\n        upsample(256, 4),  # (bs, 32, 32, 512)\\n        upsample(128, 4),  # (bs, 64, 64, 256)\\n        upsample(64, 4),   # (bs, 128, 128, 128)\\n    ]\\n    \\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=tf.random_normal_initializer(0., 0.02), activation='tanh')  # (bs, 256, 256, 3)\\n    \\n    x = inputs\\n    skips = []\\n    for down in down_stack:\\n        x = down(x)\\n        skips.append(x)\\n    \\n    skips = reversed(skips[:-1])\\n    for up, skip in zip(up_stack, skips):\\n        x = up(x)\\n        x = layers.Concatenate()([x, skip])\\n    \\n    x = last(x)\\n    return tf.keras.Model(inputs=inputs, outputs=x)\\n\\n# Discriminator: PatchGAN\\ndef Discriminator():\\n    initializer = tf.random_normal_initializer(0., 0.02)\\n    inp = layers.Input(shape=[256, 256, 3])\\n    x = downsample(64, 4, False)(inp)  \\n    x = downsample(128, 4)(x) \\n    x = downsample(256, 4)(x)  \\n    x = layers.ZeroPadding2D()(x)\\n    x = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(x)  # (bs, 31, 31, 512)\\n    x = layers.LayerNormalization()(x)\\n    x = layers.LeakyReLU()(x)\\n    x = layers.ZeroPadding2D()(x)\\n    x = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(x)  # (bs, 30, 30, 1)\\n    return tf.keras.Model(inputs=inp, outputs=x)\\n\\n# Instantiate\\n# Photo to Monet\\ngen_G = Generator()  \\n# Monet to Photo\\ngen_F = Generator()  \\n# For photos\\ndisc_X = Discriminator() \\n# For Monets\\ndisc_Y = Discriminator()\\n\""},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"**Results and Analysis (35 pts)**\n\nRun hyperparameter tuning, try different architectures for comparison, apply techniques to improve training or performance, and discuss what helped.\n\nIncludes results with tables and figures. There is an analysis of why or why not something worked well, troubleshooting, and a hyperparameter optimization procedure summary.","metadata":{}},{"cell_type":"markdown","source":"Result will speak for themselves, I will train my model on as much time that I have left on my GPU quota. Should get 50 epochs out of it, and then I will save the set of photos and submit them to be scored by kaggles MiFID scoreing system. ","metadata":{}},{"cell_type":"code","source":"'''\nLAMBDA = 10 \n\nloss_obj = tf.keras.losses.MeanSquaredError() \n\ndef discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    return (real_loss + generated_loss) * 0.5\n\ndef generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)\n\ndef calc_cycle_loss(real_image, cycled_image):\n    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * LAMBDA\n\ndef identity_loss(real_image, same_image):\n    return tf.reduce_mean(tf.abs(real_image - same_image)) * LAMBDA * 0.5 \n\n# Optimizers\ngen_G_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngen_F_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndisc_X_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndisc_Y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# Training step (use @tf.function for speed)\n@tf.function\ndef train_step(real_x, real_y):\n    with tf.GradientTape(persistent=True) as tape:\n        # Generator G: x -> y\n        fake_y = gen_G(real_x, training=True)\n        cycled_x = gen_F(fake_y, training=True)\n        \n        # Generator F: y -> x\n        fake_x = gen_F(real_y, training=True)\n        cycled_y = gen_G(fake_x, training=True)\n        \n        # Identity\n        same_x = gen_F(real_x, training=True)\n        same_y = gen_G(real_y, training=True)\n        \n        # Discriminators\n        disc_real_x = disc_X(real_x, training=True)\n        disc_real_y = disc_Y(real_y, training=True)\n        disc_fake_x = disc_X(fake_x, training=True)\n        disc_fake_y = disc_Y(fake_y, training=True)\n        \n        # Losses\n        gen_G_loss = generator_loss(disc_fake_y) + calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y) + identity_loss(real_y, same_y)\n        gen_F_loss = generator_loss(disc_fake_x) + calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y) + identity_loss(real_x, same_x)\n        total_gen_loss = gen_G_loss + gen_F_loss\n        \n        disc_X_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_Y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n    \n    # Gradients\n    gen_G_grads = tape.gradient(gen_G_loss, gen_G.trainable_variables)\n    gen_F_grads = tape.gradient(gen_F_loss, gen_F.trainable_variables)\n    disc_X_grads = tape.gradient(disc_X_loss, disc_X.trainable_variables)\n    disc_Y_grads = tape.gradient(disc_Y_loss, disc_Y.trainable_variables)\n    \n    # Apply\n    gen_G_optimizer.apply_gradients(zip(gen_G_grads, gen_G.trainable_variables))\n    gen_F_optimizer.apply_gradients(zip(gen_F_grads, gen_F.trainable_variables))\n    disc_X_optimizer.apply_gradients(zip(disc_X_grads, disc_X.trainable_variables))\n    disc_Y_optimizer.apply_gradients(zip(disc_Y_grads, disc_Y.trainable_variables))\n    \n    return total_gen_loss, disc_X_loss + disc_Y_loss\n\n# Training loop\nEPOCHS = 50 \nfor epoch in range(EPOCHS):\n    for real_monet, real_photo in train_ds:  # Note: Adjust if zipped differently\n        gen_loss, disc_loss = train_step(real_photo, real_monet)  # Photo as x, Monet as y\n    print(f\"Epoch {epoch+1}: Gen loss {gen_loss}, Disc loss {disc_loss}\")\n    \n    # Save checkpoints every 10 epochs (use tf.train.Checkpoint)\n    if (epoch + 1) % 10 == 0:\n        # TODO: Implement checkpointing\n        pass\n    \n    # Generate sample images for monitoring\n    sample_photo = next(iter(photo_ds.take(1)))\n    fake_monet = gen_G(sample_photo)\n    plt.imshow((fake_monet[0] + 1) / 2)\n    plt.show()\n    '''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.440437Z","iopub.execute_input":"2025-07-31T03:57:33.440627Z","iopub.status.idle":"2025-07-31T03:57:33.457519Z","shell.execute_reply.started":"2025-07-31T03:57:33.440612Z","shell.execute_reply":"2025-07-31T03:57:33.456891Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'\\nLAMBDA = 10 \\n\\nloss_obj = tf.keras.losses.MeanSquaredError() \\n\\ndef discriminator_loss(real, generated):\\n    real_loss = loss_obj(tf.ones_like(real), real)\\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\\n    return (real_loss + generated_loss) * 0.5\\n\\ndef generator_loss(generated):\\n    return loss_obj(tf.ones_like(generated), generated)\\n\\ndef calc_cycle_loss(real_image, cycled_image):\\n    return tf.reduce_mean(tf.abs(real_image - cycled_image)) * LAMBDA\\n\\ndef identity_loss(real_image, same_image):\\n    return tf.reduce_mean(tf.abs(real_image - same_image)) * LAMBDA * 0.5 \\n\\n# Optimizers\\ngen_G_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\\ngen_F_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\\ndisc_X_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\\ndisc_Y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\\n\\n# Training step (use @tf.function for speed)\\n@tf.function\\ndef train_step(real_x, real_y):\\n    with tf.GradientTape(persistent=True) as tape:\\n        # Generator G: x -> y\\n        fake_y = gen_G(real_x, training=True)\\n        cycled_x = gen_F(fake_y, training=True)\\n        \\n        # Generator F: y -> x\\n        fake_x = gen_F(real_y, training=True)\\n        cycled_y = gen_G(fake_x, training=True)\\n        \\n        # Identity\\n        same_x = gen_F(real_x, training=True)\\n        same_y = gen_G(real_y, training=True)\\n        \\n        # Discriminators\\n        disc_real_x = disc_X(real_x, training=True)\\n        disc_real_y = disc_Y(real_y, training=True)\\n        disc_fake_x = disc_X(fake_x, training=True)\\n        disc_fake_y = disc_Y(fake_y, training=True)\\n        \\n        # Losses\\n        gen_G_loss = generator_loss(disc_fake_y) + calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y) + identity_loss(real_y, same_y)\\n        gen_F_loss = generator_loss(disc_fake_x) + calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y) + identity_loss(real_x, same_x)\\n        total_gen_loss = gen_G_loss + gen_F_loss\\n        \\n        disc_X_loss = discriminator_loss(disc_real_x, disc_fake_x)\\n        disc_Y_loss = discriminator_loss(disc_real_y, disc_fake_y)\\n    \\n    # Gradients\\n    gen_G_grads = tape.gradient(gen_G_loss, gen_G.trainable_variables)\\n    gen_F_grads = tape.gradient(gen_F_loss, gen_F.trainable_variables)\\n    disc_X_grads = tape.gradient(disc_X_loss, disc_X.trainable_variables)\\n    disc_Y_grads = tape.gradient(disc_Y_loss, disc_Y.trainable_variables)\\n    \\n    # Apply\\n    gen_G_optimizer.apply_gradients(zip(gen_G_grads, gen_G.trainable_variables))\\n    gen_F_optimizer.apply_gradients(zip(gen_F_grads, gen_F.trainable_variables))\\n    disc_X_optimizer.apply_gradients(zip(disc_X_grads, disc_X.trainable_variables))\\n    disc_Y_optimizer.apply_gradients(zip(disc_Y_grads, disc_Y.trainable_variables))\\n    \\n    return total_gen_loss, disc_X_loss + disc_Y_loss\\n\\n# Training loop\\nEPOCHS = 50 \\nfor epoch in range(EPOCHS):\\n    for real_monet, real_photo in train_ds:  # Note: Adjust if zipped differently\\n        gen_loss, disc_loss = train_step(real_photo, real_monet)  # Photo as x, Monet as y\\n    print(f\"Epoch {epoch+1}: Gen loss {gen_loss}, Disc loss {disc_loss}\")\\n    \\n    # Save checkpoints every 10 epochs (use tf.train.Checkpoint)\\n    if (epoch + 1) % 10 == 0:\\n        # TODO: Implement checkpointing\\n        pass\\n    \\n    # Generate sample images for monitoring\\n    sample_photo = next(iter(photo_ds.take(1)))\\n    fake_monet = gen_G(sample_photo)\\n    plt.imshow((fake_monet[0] + 1) / 2)\\n    plt.show()\\n    '"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"'''from tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nfrom PIL import Image\nimport os\nimport glob\nimport zipfile\n\ndef load_image(file_path):\n    img = load_img(file_path, target_size=(256, 256))\n    img = img_to_array(img)\n    img = (img / 127.5) - 1.0  # Normalize to [-1, 1] for GAN input\n    return img\n\n# Now your code\nos.makedirs('images', exist_ok=True)\nfor i, photo_path in enumerate(photo_files):\n    photo = load_image(photo_path)\n    photo = np.expand_dims(photo, 0)  # Add batch dimension\n    generated = gen_G(photo)[0]  # Generate\n    generated = (generated * 127.5 + 127.5).numpy().astype(np.uint8)  # Denormalize to [0, 255]\n    img = Image.fromarray(generated)\n    img.save(f'images/{i+1:05d}.jpg')\n\nwith zipfile.ZipFile('images.zip', 'w') as z:\n    for f in glob.glob('images/*.jpg'):\n        z.write(f)'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T03:57:33.458117Z","iopub.execute_input":"2025-07-31T03:57:33.458368Z","iopub.status.idle":"2025-07-31T03:57:33.472994Z","shell.execute_reply.started":"2025-07-31T03:57:33.458353Z","shell.execute_reply":"2025-07-31T03:57:33.472357Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"\"from tensorflow.keras.preprocessing.image import load_img, img_to_array\\nimport numpy as np\\nfrom PIL import Image\\nimport os\\nimport glob\\nimport zipfile\\n\\ndef load_image(file_path):\\n    img = load_img(file_path, target_size=(256, 256))\\n    img = img_to_array(img)\\n    img = (img / 127.5) - 1.0  # Normalize to [-1, 1] for GAN input\\n    return img\\n\\n# Now your code\\nos.makedirs('images', exist_ok=True)\\nfor i, photo_path in enumerate(photo_files):\\n    photo = load_image(photo_path)\\n    photo = np.expand_dims(photo, 0)  # Add batch dimension\\n    generated = gen_G(photo)[0]  # Generate\\n    generated = (generated * 127.5 + 127.5).numpy().astype(np.uint8)  # Denormalize to [0, 255]\\n    img = Image.fromarray(generated)\\n    img.save(f'images/{i+1:05d}.jpg')\\n\\nwith zipfile.ZipFile('images.zip', 'w') as z:\\n    for f in glob.glob('images/*.jpg'):\\n        z.write(f)\""},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"**Conclusion (15 pts)**\n\nDiscuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future?","metadata":{}}]}